{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVn47CFzl8_d"
      },
      "source": [
        "### Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoUqLz-RdaYq"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install faiss-cpu\n",
        "!pip install python-dotenv\n",
        "!pip install langchain pypdf\n",
        "!pip install tiktoken\n",
        "!pip install pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZUdYw4nl_ZD"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jYUEs7tBl2b6"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5CAWXwP-VtT"
      },
      "source": [
        "### model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2ysutJ52pFnj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(\".env\")\n",
        "\n",
        "# Access variables\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "M8e0C9GBoBrM"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "\n",
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "z9JoNfoxo_W6",
        "outputId": "81436db5-9989-4b44-9b63-f8707477eef6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nML stands for Machine Learning and it is a subset of Artificial Intelligence that allows machines to learn and improve from data without being explicitly programmed.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"Explain ML in 1 line.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yiFzQuisoSp"
      },
      "source": [
        "### upload a pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "7BcyInOis1FZ",
        "outputId": "d44d53cb-de4c-43e2-ea52-45f11f0cc1db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-30afce5e-bee4-41d8-a873-99d69904bb27\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-30afce5e-bee4-41d8-a873-99d69904bb27\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving rag_paper.pdf to rag_paper.pdf\n",
            "Uploaded PDF file: rag_paper.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# upload a file\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# get uploaded file path\n",
        "pdf_file_path = list(uploaded_files.keys())[0]\n",
        "\n",
        "# check file extension\n",
        "if not pdf_file_path.lower().endswith(\".pdf\"):\n",
        "    raise ValueError(\"Please upload a PDF file only!\")\n",
        "\n",
        "print(f\"Uploaded PDF file: {pdf_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9lczXOM_rjbY"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "def load_document(pdf_file_path):\n",
        "  # Load PDF into Document objects\n",
        "  loader = PyPDFLoader(pdf_file_path)\n",
        "  documents = loader.load()  # returns a list of Document objects\n",
        "\n",
        "  # Check number of pages loaded\n",
        "  print(f\"Number of pages loaded: {len(documents)}\")\n",
        "\n",
        "  # Optional: preview first page\n",
        "  print(documents[0].page_content[:500])  # first 500 chars'\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOjWadfmtR6H",
        "outputId": "62abbd3b-4c7d-4595-d1c5-c3fe2c447c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of pages loaded: 21\n",
            "1\n",
            "Retrieval-Augmented Generation for Large\n",
            "Language Models: A Survey\n",
            "Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\n",
            "Wangc, and Haofen Wang a,c\n",
            "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
            "bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\n",
            "cCollege of Design and Innovation, Tongji University\n",
            "Abstract—Large Language Models (LLMs) showcase impres-\n",
            "sive capabilities \n"
          ]
        }
      ],
      "source": [
        "documents = load_document(pdf_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8aP-Kg9w-1J"
      },
      "source": [
        "### Splitting Text to Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHzwCMc9wjks"
      },
      "source": [
        "**Note:**\n",
        "I am using RecursiveCharacterTextSplitter here to capture more context from pdfs.\n",
        "\n",
        "**Info**\n",
        "RecursiveCharacterTextSplitter splits text into chunks that avoids loosing context over pages by keeping paragraphs together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "U6EnT0istgKN"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 100)\n",
        "chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nptaafbZuXnv",
        "outputId": "b4c3d717-e0ab-4e3a-9e49-b69cdf2885c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'rag_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'rag_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1'}, page_content='Abstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-')]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v80-aJ0kxDax"
      },
      "source": [
        "### Vector Index/DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR3SGje2yW9h"
      },
      "source": [
        "FAISS: Facebook AI Similarity Search --> a powerful library for similarity search and clustering of dense vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvlkTRhHuZS7",
        "outputId": "33a24a89-96dd-46c9-808a-62ee44ecfcce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3156445715.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings(api_key = OPENAI_API_KEY)\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings  # many others are availabe for other models too.\n",
        "\n",
        "# creating embeddings\n",
        "embeddings = OpenAIEmbeddings(api_key = OPENAI_API_KEY)\n",
        "db = FAISS.from_documents(documents=chunks, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7yqn6XHdxmhq"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "      Given the following conversation and follow up question, get insights about the question and give short\n",
        "      precise answer.\n",
        "      {chat_history}\n",
        "      Follow up Input: {question}\n",
        "      Insights:\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(), condense_question_prompt= CONDENSE_QUESTION_PROMPT,\n",
        "                                           return_source_documents=True, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UioS1k4i6C8j",
        "outputId": "707066ab-285c-4db6-8006-7ffe9ae1999c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " RAG is a research paradigm that has evolved over time and is now categorized into three stages: Naive RAG, Advanced RAG, and Modular RAG. It has shown to be cost-effective and outperform native LLMs, but also has limitations. The development of Advanced RAG and Modular RAG aim to address these limitations. RAG is continuously expanding its application scope into multimodal domains and has gained interest from academic and industrial sectors.\n"
          ]
        }
      ],
      "source": [
        "chat_history= []\n",
        "query = \"What is RAG?.\"\n",
        "result = qa.invoke(input =\n",
        "          {\n",
        "              \"question\":query,\n",
        "              \"chat_history\": chat_history\n",
        "          })\n",
        "\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99bJJaAc3wvS",
        "outputId": "0ac5d2bc-5ae2-42d2-a666-f41a61e6ed1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " RAG can be used for addressing complex problems and integrative or summary questions that require reading a large amount of material to answer. It can also be used in multimodal domains, such as interpreting and processing images, videos, and code, and for improving precision and flexibility in queries. Additionally, RAG can be used for decision-making and autonomous judgment capabilities in generating accurate responses. \n"
          ]
        }
      ],
      "source": [
        "chat_history= []\n",
        "query = \"What can rag be used for?.\"\n",
        "result = qa.invoke(input =\n",
        "          {\n",
        "              \"question\":query,\n",
        "              \"chat_history\": chat_history\n",
        "          })\n",
        "\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRCvgIoB4MmE",
        "outputId": "a5c80aeb-18b1-40f6-e2c1-e0841e4c8017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The workflow of RAG involves a flexible orchestration that showcases the benefits of adaptive retrieval through techniques such as FLARE and Self-RAG. This approach evaluates the necessity of retrieval based on different scenarios and allows for integration with other technologies like fine-tuning or reinforcement learning. RAG's application scope is also expanding into multimodal domains, making it applicable to diverse data forms like images, videos, and code. The evaluation of RAG models is a primary objective in the field of NLP, with a focus on optimizing performance across various application scenarios. The core task of RAG is Question Answering, including traditional single-hop/multi-hop QA, multiple-choice QA, and open-domain QA. \n"
          ]
        }
      ],
      "source": [
        "chat_history= []\n",
        "query = \"Can you tell more about the workflow of rag?.\"\n",
        "result = qa.invoke(input =\n",
        "          {\n",
        "              \"question\":query,\n",
        "              \"chat_history\": chat_history\n",
        "          })\n",
        "\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CMNXz78RoDj"
      },
      "source": [
        "### Setup Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Q2HroRMvRpmf"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "import os\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A2cPr9k0SL7a"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "  index_name = \"pdf-ragbot\"\n",
        "\n",
        "  if not pc.has_index(index_name):\n",
        "      pc.create_index_for_model(\n",
        "          name=index_name,\n",
        "          cloud=\"aws\",\n",
        "          region=\"us-east-1\",\n",
        "          embed={\n",
        "              \"model\": \"llama-text-embed-v2\",  # works for OpenAI embeddings but is much bigger\n",
        "              \"field_map\": {\"text\": \"chunk_text\"}\n",
        "          }\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfJAFuzqY2fp",
        "outputId": "86b9abf0-69e0-4f17-de74-486cdf5b19f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1024,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index = pc.Index(\"pdf-ragbot\")\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Wq_mH020YQER"
      },
      "outputs": [],
      "source": [
        "# Create with exact dimensions\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "\n",
        "spec = ServerlessSpec(cloud=cloud,region=region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mZW1KCa9YijQ"
      },
      "outputs": [],
      "source": [
        "index_name = 'rag-chatbot'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNSotX1WYmVw",
        "outputId": "8afc86e6-ed49-4fda-e5ec-f94334fbcdbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if index_name not in pc.list_indexes().names():\n",
        "  pc.create_index(index_name, dimension=1536,metric='cosine',spec=spec)\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpaFpyjUZJaA"
      },
      "source": [
        "### Authenticate Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHgNsPOwZI44"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone_notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "FqM6fI6SY_Qn",
        "outputId": "4e55aa2c-7210-4ed6-931c-324675ab4e38"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<script type=\"text/javascript\" src=\"https://connect.pinecone.io/embed.js\"></script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pinecone_notebooks.colab import Authenticate\n",
        "Authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb5qKf7aZunA"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3rycu7UZqaw",
        "outputId": "58f9cdfe-c0a7-4c5c-c004-0ddbf32fc764"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
          ]
        }
      ],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "docsearch = PineconeVectorStore.from_documents(chunks,embeddings, index_name = index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb_qJP9EaNx3",
        "outputId": "b5e20928-a8f1-4f9c-ab0c-dcaf76a4e8db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x78593e8f6f60>, search_kwargs={})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QUFNmpQeaSUQ"
      },
      "outputs": [],
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
        "                                           retriever=docsearch.as_retriever(),\n",
        "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
        "                                           return_source_documents=True,\n",
        "                                           verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "FekHmF1ScaJG"
      },
      "outputs": [],
      "source": [
        "def append_history(chat_history,query, answer):\n",
        "  \"\"\"\n",
        "    Append new prompt query and answer to chat history.\n",
        "  \"\"\"\n",
        "  chat_history.append((\"user\", query))\n",
        "  chat_history.append((\"assistant\", result[\"answer\"]))\n",
        "  return chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXYOziHqaV1g",
        "outputId": "bf1936b6-43d0-4b19-b796-ada4a4fd6104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " RAG stands for Retrieval-Augmented Generation and is a research paradigm that has three stages: Naive, Advanced, and Modular RAG. RAG is continuously evolving and has applications in various domains such as images, videos, and code. It is cost-effective and has shown to surpass the performance of native LLMs, but also has limitations that have led to the development of Advanced RAG and Modular RAG. As RAG's application scope expands, there is a need for refining evaluation methodologies to accurately assess its performance and contributions to the AI research and development community.\n"
          ]
        }
      ],
      "source": [
        "chat_history = []\n",
        "query = \"\"\"What is RAG?\"\"\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d03YVNRbO3H",
        "outputId": "eabd8eea-eed0-40aa-f9a5-bc33aa4990a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('user', 'What is RAG?'),\n",
              " ('assistant',\n",
              "  \" RAG stands for Retrieval-Augmented Generation and is a research paradigm that has three stages: Naive, Advanced, and Modular RAG. RAG is continuously evolving and has applications in various domains such as images, videos, and code. It is cost-effective and has shown to surpass the performance of native LLMs, but also has limitations that have led to the development of Advanced RAG and Modular RAG. As RAG's application scope expands, there is a need for refining evaluation methodologies to accurately assess its performance and contributions to the AI research and development community.\")]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = append_history(chat_history,query, result[\"answer\"])\n",
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usXucG-eabGw",
        "outputId": "1f08a733-59c3-4b97-e117-cf729d6291d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \"What are the reasons for the growing adoption of RAG in the field of NLP and what advantages does it offer? How does the development of Advanced RAG and Modular RAG address the limitations of Naive RAG? How can evaluation methodologies be refined to accurately assess RAG's contributions to the AI research and development community?\"\n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"Why use RAG?.\"\"\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDkOwFHgcvef",
        "outputId": "ef28c34d-5f7e-4a89-a943-8277d8d665e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('user', 'What is RAG?'),\n",
              " ('assistant',\n",
              "  \" RAG stands for Retrieval-Augmented Generation and is a research paradigm that has three stages: Naive, Advanced, and Modular RAG. RAG is continuously evolving and has applications in various domains such as images, videos, and code. It is cost-effective and has shown to surpass the performance of native LLMs, but also has limitations that have led to the development of Advanced RAG and Modular RAG. As RAG's application scope expands, there is a need for refining evaluation methodologies to accurately assess its performance and contributions to the AI research and development community.\"),\n",
              " ('user', 'Why use RAG?.'),\n",
              " ('assistant',\n",
              "  ' \"What are the reasons for the growing adoption of RAG in the field of NLP and what advantages does it offer? How does the development of Advanced RAG and Modular RAG address the limitations of Naive RAG? How can evaluation methodologies be refined to accurately assess RAG\\'s contributions to the AI research and development community?\"')]"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = append_history(chat_history,query, result[\"answer\"])\n",
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
